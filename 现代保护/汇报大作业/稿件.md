老师同学们大家上午好，我是第11组的尚子轩，我介绍的题目是基于大规模深度强化学习的Dota2游戏AI，对应的是OpenAI公司推出的OpenAI Five的项目，讲一下这篇论文的大致思路

## Dota 2 游戏

我们的背景就是Dota 2，以Dota2这款MOBA类游戏作为研究对象，类似于王者荣耀的那一类游戏，而此类游戏想要实现AI控制是相当困难的，这种困难程度是远高于在17年击败围棋冠军且同为强化学习框架下的Alpha go（我没记错的话），具体细节已经列出，决策序列长方面，一局Dota2游戏的平均时长为45分钟，运行在30帧每秒，4帧进行一次决策，那一局就要进行2w次以上的动作，远高于围棋象棋将近100步的动作，再者，由于存在战争迷雾，玩家无法观测到系统的整个状态信息，只能通过不完全的数据及进行决策，在可选动作和可观察到的所有状态方面，Dota2的两个数值也是远高于象棋围棋的，离散化后的动作有8k到8w种，棋类则是几十到几百左右

## 强化学习概念

然后需要了解一下这个强化学习，

1. 强化学习本身属于机器学习，而机器学习如果按照监督学习和非监督学习来分类，强化学习就属于第三类，
2. 与需要大量数据集的深度学习对比，强化学习不需要太多的数据，而是需要一个能产生数据的与之交互的环境，游戏，像那种像素类Atari游戏，是验证强化学习算法好坏最理想的环境；
3. 而相比于神经网络反向传播算法可以通过链式法则进行数学推导，强化学习算法收敛性可通过贝尔曼方程进行证明，最后就是强化学习算法可以与神经网络算法进行结合，演变成目前研究挺热门的深度强化学习

## 强化学习结构

然后就是这个强化学习的结构，首先是环境，之前说过了，应该是区别于监督学习和无监督学习最重要的一部分，然后是算法，强化学习算法有很多种，什么Q-learning，Sarsa，DQN，DDPG，SAC，A3C，PPO，TRPO，由于PPO应该是目前最容易收敛也是最为流行的算法，本文采用这种算法

状态空间在Dota2游戏中可以理解成比如说英雄的血量，位置，技能冷却等等可以看到的，

动作空间就是英雄的移动，攻击，释放技能等等这种可以选择释放的，

奖励可以理解成你击败了敌方英雄，给你一波奖励，推了一塔，给你一波奖励等等这样的

## 模型简化

然后我们对游戏的具体模型作出了一系列的简化假设，117个英雄只拿出17个做训练，有个分身斧的装备可以让英雄分身，直接ban掉这个道具，观测也是，直接从游戏后台读信息而不是根据显示器上的画面，动作方面，游戏里面需要鼠标键盘操作拖拽的动作直接使用脚本实现

## 模型学习目标

目标也是显而易见的，“学会玩Dota 2并达到超越人类的水平”，现在就要祭出这张在OpenAI Five官网找来的图了，横坐标表示时间2016-2019年，纵坐标是一个叫做TrueSkill的数值，TrueSkill就可以理解为，假设你以玩家的身份去打Dota2的排位，游戏水平不发生变化的情况下，且打的次数足够多，就会达到一个动态平衡点，这个点的段位分数就可以理解为你目前的TrueSkill，也可以用来直接评价你的模型现在到底有多厉害

## 奖励设计

奖励设计如图所示，自上而下，赢对应5的奖励，英雄阵亡-1的奖励，还有后面一系列的，拿钱花钱都有奖励，还有就是基地的血量啦，1，2塔，高地塔，门牙塔的血量甚至到超级兵都有对应奖励

还引入了博弈论里的零和博弈，也就是一方的奖励就是对手的惩罚，时间惩罚使得到了游戏后期，拿钱拿经验等等奖励没那么重要，就给他的数值随时间进行衰减

## 训练系统

把观测到的数据一股脑的转换成一维的数据，然后经过一个长短期记忆网络层输出值函数（强化学习算法需要的）和对应的动作（直接控制英雄做出的），具体怎么转成一维的可以看下一张图，内容比较多，大致是小地图成分的进行红色处理，也就是过一下卷积层，黄色属于连续数值，过归一层，类似这些细节

## 手术机制

到这里我们就假设我们的模型已经训练出来了，但其实并不是特别符合我们预期的，其中一个原因就是我们在之前进行了简化，减了有100个英雄吧好像，一些道具，还有另外一个原因在于V社，也就是开发Dota2游戏的这个公司，他为了维护游戏的平衡性，会不断地更新游戏，这就导致我们训练好的跟最新版本的Dota2环境是不一致的，自然就不容易得到好的结果了。为了解决这个问题，我们可能就会想，那要不重新训练一遍？从实际出发，那是基本不可能的，我们的上一次训练，在12万8k个CPU，GPU集群上训练，训练了十个月才训练出的，进行一次小版本更新我们就要再训练，所以说我们引入手术机制，也就是Surgery，它的本质也很简单，就是我们直接对之前搭好的神经网络结构进行增删改等一通操作，方法非常多，在次我就举一个我比较感兴趣的例子，就是在训练中，我们慢慢地，有过渡地按照一个速度去改变环境，来避免环境突变引起模型能力的突变，引入退火的思想，如果目前这个速度训练过程中能力发生下降，就进一步降低速度，以求模型训练可以平稳过渡

## 结果及意义

结果这一块可能就比较显而易见了，在19年4月13日击败当时dota2TI8（18年）的世界冠军OG，值得注意的是，OG同时也是当年19年的卫冕冠军，随后就是跟不同的队伍进行比赛，胜率99.4%，可能人们普遍认为AI在反应时间上会占优势，但由于OpenAI Five存在延时机制，换算后反应时间大概在167~267ms，正常人平均也就250ms，所以基本可以认为还是比较公平的

## 意义

他在与OG进行比赛时采取了很多当时职业队伍都没有想过没有尝试过的比较新奇的战术，部分战术思路在近几年来也被职业队伍所沿用，有的战术看上去很怪，但实际应用时却十分有效

这就是我的所有介绍内容了



